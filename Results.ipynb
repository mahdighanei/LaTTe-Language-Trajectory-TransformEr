{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mghanei3/miniconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-04-24 14:28:36.451403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# import os # I think for cpu\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "def evaluate_model(model, x_t, y_t):\n",
    "\n",
    "    print(\"\\nwith next waypoint prediction\")\n",
    "    result_eval = model.evaluate(x_t,y_t)\n",
    "    print(\"MSE: \",result_eval)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"with autoregressive generation:\")\n",
    "\n",
    "    pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "    result_gen = np.average((y_t - pred[:,1:,:])**2)\n",
    "    print(\"MSE: \",result_gen)\n",
    "    print(\"Trajectory metrics:\")\n",
    "    metrics, metrics_h = compute_metrics(y_t.numpy()[:,:,:],pred[:,1:,:])\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset size experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cpu\n",
      "loading dataset:  latte_100k_lf ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (96718, 953) \tY: (96718, 160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 14:29:09.583310: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-04-24 14:29:09.583483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-04-24 14:29:09.635395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.636003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2070 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.305GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2024-04-24 14:29:09.636059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-04-24 14:29:09.638208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-04-24 14:29:09.638285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-04-24 14:29:09.639908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-04-24 14:29:09.639962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-04-24 14:29:09.641773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-04-24 14:29:09.642690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-04-24 14:29:09.645117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-04-24 14:29:09.645195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.645540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.645822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2024-04-24 14:29:09.646168: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 14:29:09.646480: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-04-24 14:29:09.646538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.646834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2070 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.305GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2024-04-24 14:29:09.646860: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-04-24 14:29:09.646878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-04-24 14:29:09.646893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-04-24 14:29:09.646908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-04-24 14:29:09.646919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-04-24 14:29:09.646933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-04-24 14:29:09.646948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-04-24 14:29:09.646963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-04-24 14:29:09.647004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.647317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-24 14:29:09.647596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#============================== load dataset ==========================================\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X,Y, data \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatte_100k_lf\u001b[39m\u001b[38;5;124m\"\u001b[39m, filter_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, base_path\u001b[38;5;241m=\u001b[39mdata_folder)\n\u001b[0;32m----> 7\u001b[0m X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val \u001b[38;5;241m=\u001b[39m \u001b[43mmr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((mr\u001b[38;5;241m.\u001b[39mprepare_x(X_test),\n\u001b[1;32m     10\u001b[0m                                                     list_to_wp_seq(y_test,d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     11\u001b[0m                                                     X_test[:,embedding_indices]))\u001b[38;5;241m.\u001b[39mbatch(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m g \u001b[38;5;241m=\u001b[39m generator(test_dataset,stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Mahdi/research/code/llmclass/LaTTe-Language-Trajectory-TransformEr/src/motion_refiner_4D.py:196\u001b[0m, in \u001b[0;36mMotion_refiner.split_dataset\u001b[0;34m(self, X, Y, test_size, val_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, val_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m     \u001b[43mreset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# Split the data: 70% train 20% test 10% validation\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     n_samples, input_size \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Mahdi/research/code/llmclass/LaTTe-Language-Trajectory-TransformEr/src/functions.py:860\u001b[0m, in \u001b[0;36mreset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    858\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(seed)\n\u001b[1;32m    859\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF_DETERMINISTIC_OPS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 860\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_global_generator(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/ops/stateful_random_ops.py:453\u001b[0m, in \u001b[0;36mGenerator.from_seed\u001b[0;34m(cls, seed, alg)\u001b[0m\n\u001b[1;32m    451\u001b[0m alg \u001b[38;5;241m=\u001b[39m _convert_alg_to_int(alg)\n\u001b[1;32m    452\u001b[0m state \u001b[38;5;241m=\u001b[39m create_rng_state(seed, alg)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/ops/stateful_random_ops.py:372\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self, copy_from, state, alg)\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_var \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m   state \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_state_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m   _check_state_shape(state\u001b[38;5;241m.\u001b[39mshape, alg)\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_variable(state, dtype\u001b[38;5;241m=\u001b[39mSTATE_TYPE,\n\u001b[1;32m    375\u001b[0m                                           trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/ops/stateful_random_ops.py:235\u001b[0m, in \u001b[0;36m_convert_to_state_tensor\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;66;03m# to avoid out-of-range error from ops.convert_to_tensor\u001b[39;00m\n\u001b[1;32m    234\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_uint_to_int, t))\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTATE_TYPE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1537\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1540\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1543\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    275\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    279\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;124;03m\"\"\"Implementation of eager constant.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:97\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m---> 97\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/eager/context.py:526\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_tfrt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetTfrt(opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_tfrt)\n\u001b[0;32m--> 526\u001b[0m   context_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_NewContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
      "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
     ]
    }
   ],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             0      \\\n",
      "input_traj       [[0.09, 0.09], [0.098278782956566, 0.098476967...   \n",
      "output_traj      [[0.09, 0.09], [0.09698759510694001, 0.0997808...   \n",
      "text                                walk much closer to the table.   \n",
      "obj_names                                       [table, chair, TV]   \n",
      "obj_poses        [[0.32, 0.46, 0.29], [0.49, 0.3000000000000000...   \n",
      "obj_in_text                                                  table   \n",
      "token_text       [101, 3328, 2172, 3553, 2000, 1996, 2795, 1012...   \n",
      "token_obj_name   [[49406, 2175, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2374, 1238, 6377, 531, 518, 2175, 269...   \n",
      "similarity       [[0.879068732261657, 0.783072352409362, 0.7772...   \n",
      "\n",
      "                                                             1      \\\n",
      "input_traj       [[0.09, 0.09], [0.090834695449216, 0.104829838...   \n",
      "output_traj      [[0.09, 0.09], [0.089854590820949, 0.105593076...   \n",
      "text                        stay a lot further away from the wall.   \n",
      "obj_names                                        [wall, car, wall]   \n",
      "obj_poses        [[0.25, 0.47000000000000003, 0.68], [0.36, 0.2...   \n",
      "obj_in_text                                                   wall   \n",
      "token_text       [101, 2994, 1037, 2843, 2582, 2185, 2013, 1996...   \n",
      "token_obj_name   [[49406, 2569, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2277, 320, 1954, 5583, 1520, 633, 518...   \n",
      "similarity       [[0.858827948570251, 0.7838994860649101, 0.858...   \n",
      "\n",
      "                                                             2      \\\n",
      "input_traj       [[0.09, 0.09], [0.09434675188321501, 0.1038775...   \n",
      "output_traj      [[0.09, 0.09], [0.093746315432835, 0.105142678...   \n",
      "text                                stay much closer to the table.   \n",
      "obj_names                                       [table, TV, table]   \n",
      "obj_poses        [[0.30000000000000004, 0.79, 0.31], [0.29, 0.5...   \n",
      "obj_in_text                                                  table   \n",
      "token_text       [101, 2994, 2172, 3553, 2000, 1996, 2795, 1012...   \n",
      "token_obj_name   [[49406, 2175, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2277, 1238, 6377, 531, 518, 2175, 269...   \n",
      "similarity       [[0.8966823816299431, 0.771414816379547, 0.896...   \n",
      "\n",
      "                                                             3      \\\n",
      "input_traj       [[0.09, 0.09], [0.10233467058735801, 0.0972337...   \n",
      "output_traj      [[0.09, 0.09], [0.100767639795777, 0.098587087...   \n",
      "text                        pass much further away from the table.   \n",
      "obj_names                                      [car, table, chair]   \n",
      "obj_poses                 [[0.33, 0.63, 0.28], [0.68, 0.26, 0.39]]   \n",
      "obj_in_text                                                  table   \n",
      "token_text       [101, 3413, 2172, 2582, 2185, 2013, 1996, 2795...   \n",
      "token_obj_name   [[49406, 1615, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 3511, 1238, 5583, 1520, 633, 518, 217...   \n",
      "similarity       [[0.7791268825531, 0.8735635280609131, 0.79299...   \n",
      "\n",
      "                                                             4      \\\n",
      "input_traj       [[0.09, 0.09], [0.09827882828282801, 0.0982788...   \n",
      "output_traj      [[0.09, 0.09], [0.09948408679500001, 0.0970912...   \n",
      "text                             pass further away from the table.   \n",
      "obj_names                                   [table, bycicle, rock]   \n",
      "obj_poses        [[0.48, 0.39, 0.24], [0.79, 0.65, 0.7000000000...   \n",
      "obj_in_text                                                  table   \n",
      "token_text       [101, 3413, 2582, 2185, 2013, 1996, 2795, 1012...   \n",
      "token_obj_name   [[49406, 2175, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 3511, 5583, 1520, 633, 518, 2175, 269...   \n",
      "similarity       [[0.8733779191970821, 0.7497172951698301, 0.81...   \n",
      "\n",
      "                                                             5      \\\n",
      "input_traj       [[0.09, 0.09], [0.10752342063384801, 0.1029477...   \n",
      "output_traj      [[0.09, 0.09], [0.107442058565409, 0.101999495...   \n",
      "text                                               go to the back.   \n",
      "obj_names                               [bycicle, cellphone, rock]   \n",
      "obj_poses                 [[0.61, 0.34, 0.63], [0.54, 0.43, 0.61]]   \n",
      "obj_in_text                                                          \n",
      "token_text                [101, 2175, 2000, 1996, 2067, 1012, 102]   \n",
      "token_obj_name   [[49406, 1713, 14539, 579, 49407, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 861, 531, 518, 893, 269, 49407, 0, 0,...   \n",
      "similarity       [[0.7966516613960261, 0.8403040766716001, 0.88...   \n",
      "\n",
      "                                                             6      \\\n",
      "input_traj       [[0.09, 0.09], [0.101804253390311, 0.099403146...   \n",
      "output_traj      [[0.09, 0.09], [0.101455079792984, 0.100024736...   \n",
      "text                             pass a little closer to the wall.   \n",
      "obj_names                                   [car, wall, cellphone]   \n",
      "obj_poses                 [[0.53, 0.58, 0.71], [0.54, 0.59, 0.29]]   \n",
      "obj_in_text                                                   wall   \n",
      "token_text       [101, 3413, 1037, 2210, 3553, 2000, 1996, 2813...   \n",
      "token_obj_name   [[49406, 1615, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 3511, 320, 1274, 6377, 531, 518, 2569...   \n",
      "similarity       [[0.7834314703941341, 0.8582295775413511, 0.81...   \n",
      "\n",
      "                                                             7      \\\n",
      "input_traj       [[0.09, 0.09], [0.09957004682666501, 0.1025419...   \n",
      "output_traj      [[0.09, 0.09], [0.101071281699598, 0.101556057...   \n",
      "text                                pass a lot closer to the tree.   \n",
      "obj_names                                  [tree, wall, cellphone]   \n",
      "obj_poses        [[0.6000000000000001, 0.75, 0.4700000000000000...   \n",
      "obj_in_text                                                   tree   \n",
      "token_text       [101, 3413, 1037, 2843, 3553, 2000, 1996, 3392...   \n",
      "token_obj_name   [[49406, 2677, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 3511, 320, 1954, 6377, 531, 518, 2677...   \n",
      "similarity       [[0.8971594572067261, 0.763848304748535, 0.775...   \n",
      "\n",
      "                                                             8      \\\n",
      "input_traj       [[0.09, 0.09], [0.09685308924819701, 0.1071962...   \n",
      "output_traj      [[0.09, 0.09], [0.088946848916665, 0.111096998...   \n",
      "text                               drive further away from the TV.   \n",
      "obj_names                                         [TV, table, car]   \n",
      "obj_poses        [[0.36, 0.73, 0.48], [0.5700000000000001, 0.34...   \n",
      "obj_in_text                                                     TV   \n",
      "token_text       [101, 3298, 2582, 2185, 2013, 1996, 2694, 1012...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2620, 5583, 1520, 633, 518, 1583, 269...   \n",
      "similarity       [[0.880900084972381, 0.7853838801383971, 0.820...   \n",
      "\n",
      "                                                             9      ...  \\\n",
      "input_traj       [[0.09, 0.09], [0.099006031947131, 0.098306096...  ...   \n",
      "output_traj      [[0.09, 0.09], [0.09803538377699, 0.0995792310...  ...   \n",
      "text                             walk further away from the chair.  ...   \n",
      "obj_names                                       [car, wall, chair]  ...   \n",
      "obj_poses                 [[0.73, 0.33, 0.73], [0.38, 0.51, 0.48]]  ...   \n",
      "obj_in_text                                                  chair  ...   \n",
      "token_text       [101, 3328, 2582, 2185, 2013, 1996, 3242, 1012...  ...   \n",
      "token_obj_name   [[49406, 1615, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...   \n",
      "token_clip_text  [[49406, 2374, 5583, 1520, 633, 518, 4269, 269...  ...   \n",
      "similarity       [[0.7658304572105401, 0.7742593288421631, 0.86...  ...   \n",
      "\n",
      "                                                             10309  \\\n",
      "input_traj       [[0.09, 0.09], [0.10179312064100701, 0.0985995...   \n",
      "output_traj      [[0.09, 0.09], [0.10081354704999801, 0.0723220...   \n",
      "text                               drive further away from the TV.   \n",
      "obj_names                                    [TV, cellphone, wall]   \n",
      "obj_poses        [[0.43, 0.73, 0.7000000000000001], [0.5, 0.27,...   \n",
      "obj_in_text                                                     TV   \n",
      "token_text       [101, 3298, 2582, 2185, 2013, 1996, 2694, 1012...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2620, 5583, 1520, 633, 518, 1583, 269...   \n",
      "similarity       [[0.880900084972381, 0.8155547380447381, 0.780...   \n",
      "\n",
      "                                                             10310  \\\n",
      "input_traj       [[0.09, 0.09], [0.10179312064100701, 0.0985995...   \n",
      "output_traj      [[0.09, 0.09], [0.10276007234548401, 0.0970849...   \n",
      "text                              pass further away from the wall.   \n",
      "obj_names                                    [TV, cellphone, wall]   \n",
      "obj_poses        [[0.43, 0.73, 0.7000000000000001], [0.5, 0.27,...   \n",
      "obj_in_text                                                   wall   \n",
      "token_text       [101, 3413, 2582, 2185, 2013, 1996, 2813, 1012...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 3511, 5583, 1520, 633, 518, 2569, 269...   \n",
      "similarity       [[0.801138758659362, 0.8265104293823241, 0.850...   \n",
      "\n",
      "                                                             10311  \\\n",
      "input_traj       [[0.09, 0.09], [0.10179312064100701, 0.0985995...   \n",
      "output_traj      [[0.09, 0.09], [0.10484393449528301, 0.0869286...   \n",
      "text                       stay a little further away from the TV.   \n",
      "obj_names                                    [TV, cellphone, wall]   \n",
      "obj_poses        [[0.43, 0.73, 0.7000000000000001], [0.5, 0.27,...   \n",
      "obj_in_text                                                     TV   \n",
      "token_text       [101, 2994, 1037, 2210, 2582, 2185, 2013, 1996...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2277, 320, 1274, 5583, 1520, 633, 518...   \n",
      "similarity       [[0.900242805480957, 0.8157942891120911, 0.786...   \n",
      "\n",
      "                                                             10312  \\\n",
      "input_traj       [[0.09, 0.09], [0.10179312064100701, 0.0985995...   \n",
      "output_traj      [[0.09, 0.09], [0.103563818687273, 0.096428975...   \n",
      "text                            stay much closer to the cellphone.   \n",
      "obj_names                                    [TV, cellphone, wall]   \n",
      "obj_poses        [[0.43, 0.73, 0.7000000000000001], [0.5, 0.27,...   \n",
      "obj_in_text                                              cellphone   \n",
      "token_text       [101, 2994, 2172, 3553, 2000, 1996, 3526, 9864...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2277, 1238, 6377, 531, 518, 39029, 26...   \n",
      "similarity       [[0.7988538146018981, 0.917698800563812, 0.774...   \n",
      "\n",
      "                                                             10313  \\\n",
      "input_traj       [[0.09, 0.09], [0.10179312064100701, 0.0985995...   \n",
      "output_traj      [[0.09, 0.09], [0.10028989661380401, 0.0995705...   \n",
      "text                    walk very further away from the cellphone.   \n",
      "obj_names                                    [TV, cellphone, wall]   \n",
      "obj_poses        [[0.43, 0.73, 0.7000000000000001], [0.5, 0.27,...   \n",
      "obj_in_text                                              cellphone   \n",
      "token_text       [101, 3328, 2200, 2582, 2185, 2013, 1996, 3526...   \n",
      "token_obj_name   [[49406, 1583, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2374, 1070, 5583, 1520, 633, 518, 390...   \n",
      "similarity       [[0.7740904688835141, 0.873367607593536, 0.777...   \n",
      "\n",
      "                                                             10314  \\\n",
      "input_traj       [[0.09, 0.09], [0.10333527115544701, 0.1002479...   \n",
      "output_traj      [[0.09, 0.09], [0.10612782109890201, 0.0957496...   \n",
      "text                        keep a bigger distance from the chair.   \n",
      "obj_names                                  [chair, wall, computer]   \n",
      "obj_poses        [[0.6000000000000001, 0.4, 0.48], [0.58, 0.700...   \n",
      "obj_in_text                                                  chair   \n",
      "token_text       [101, 2562, 1037, 7046, 3292, 2013, 1996, 3242...   \n",
      "token_obj_name   [[49406, 4269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 1726, 320, 6806, 7964, 633, 518, 4269...   \n",
      "similarity       [[0.875572919845581, 0.7373198866844171, 0.762...   \n",
      "\n",
      "                                                             10315  \\\n",
      "input_traj       [[0.09, 0.09], [0.10333527115544701, 0.1002479...   \n",
      "output_traj      [[0.09, 0.09], [0.104569163901553, 0.098513960...   \n",
      "text                     stay much further away from the computer.   \n",
      "obj_names                                  [chair, wall, computer]   \n",
      "obj_poses        [[0.6000000000000001, 0.4, 0.48], [0.58, 0.700...   \n",
      "obj_in_text                                               computer   \n",
      "token_text       [101, 2994, 2172, 2582, 2185, 2013, 1996, 3274...   \n",
      "token_obj_name   [[49406, 4269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2277, 1238, 5583, 1520, 633, 518, 116...   \n",
      "similarity       [[0.768370449542999, 0.7942343950271601, 0.921...   \n",
      "\n",
      "                                                             10316  \\\n",
      "input_traj       [[0.09, 0.09], [0.10333527115544701, 0.1002479...   \n",
      "output_traj      [[0.09, 0.09], [0.105758293995989, 0.096334018...   \n",
      "text                       walk a bit further away from the chair.   \n",
      "obj_names                                  [chair, wall, computer]   \n",
      "obj_poses        [[0.6000000000000001, 0.4, 0.48], [0.58, 0.700...   \n",
      "obj_in_text                                                  chair   \n",
      "token_text       [101, 3328, 1037, 2978, 2582, 2185, 2013, 1996...   \n",
      "token_obj_name   [[49406, 4269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2374, 320, 2010, 5583, 1520, 633, 518...   \n",
      "similarity       [[0.8538662195205681, 0.7695207595825191, 0.79...   \n",
      "\n",
      "                                                             10317  \\\n",
      "input_traj       [[0.09, 0.09], [0.10333527115544701, 0.1002479...   \n",
      "output_traj      [[0.09, 0.09], [0.10053064171070601, 0.1046652...   \n",
      "text                            walk a lot closer to the computer.   \n",
      "obj_names                                  [chair, wall, computer]   \n",
      "obj_poses        [[0.6000000000000001, 0.4, 0.48], [0.58, 0.700...   \n",
      "obj_in_text                                               computer   \n",
      "token_text       [101, 3328, 1037, 2843, 3553, 2000, 1996, 3274...   \n",
      "token_obj_name   [[49406, 4269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "token_clip_text  [[49406, 2374, 320, 1954, 6377, 531, 518, 1163...   \n",
      "similarity       [[0.746647298336029, 0.7830207347869871, 0.883...   \n",
      "\n",
      "                                                             10318  \n",
      "input_traj       [[0.09, 0.09], [0.10333527115544701, 0.1002479...  \n",
      "output_traj      [[0.09, 0.09], [0.10053064171070601, 0.1046652...  \n",
      "text                             walk much closer to the computer.  \n",
      "obj_names                                  [chair, wall, computer]  \n",
      "obj_poses        [[0.6000000000000001, 0.4, 0.48], [0.58, 0.700...  \n",
      "obj_in_text                                               computer  \n",
      "token_text       [101, 3328, 2172, 3553, 2000, 1996, 3274, 1012...  \n",
      "token_obj_name   [[49406, 4269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "token_clip_text  [[49406, 2374, 1238, 6377, 531, 518, 11639, 65...  \n",
      "similarity       [[0.7572551369667051, 0.78611797094345, 0.8870...  \n",
      "\n",
      "[10 rows x 10319 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'obj_poses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/_libs/index.pyx:155\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'obj_poses'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m foo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(foo)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobj_poses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'obj_poses'"
     ]
    }
   ],
   "source": [
    "# print(X_test[0])\n",
    "# print(X_test.shape)\n",
    "import pandas as pd\n",
    "foo = pd.read_json(\"data/data.json\")\n",
    "print(foo)\n",
    "print(foo.iloc[0]['obj_poses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': True, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh'}\n",
      "loading weights:  models/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh.h5\n",
      "DONE\n",
      "model loaded\n",
      "\n",
      "with next waypoint prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 14:23:20.232017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-04-24 14:23:20.256705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/605 [==============================] - 93s 150ms/step - loss: 1.0956e-04\n",
      "MSE:  0.00011123557487735525\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 1/39 [00:27<17:21, 27.40s/it]"
     ]
    }
   ],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "# model_path = models_folder+\"FINAL_dataset_size_aug_fixsteps/\"\n",
    "models_folder = \"models/\"\n",
    "model_path = models_folder\n",
    "\n",
    "model_names =  [\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.01-augment:0.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.01-augment:1.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.1-augment:0.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.1-augment:1.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:0.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:1.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:0.h5\",\n",
    "                \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"]\n",
    "\n",
    "model_names = [\"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh.h5\"]\n",
    "models_metrics = {}\n",
    "for model_name in model_names:\n",
    "\n",
    "    model_file = model_path+model_name\n",
    "    # param = file_name2dict(model_file,delimiter=\"-\",show=False)\n",
    "    # print(\"======================================================================================================\")\n",
    "    # print(\"Dataset size: \",str(100.0*float(param[\"sf\"])),\"%\\t\",\"augmentation: \",\"ON \"if param['augment']==1 else \"OFF\", \"\\n\")\n",
    "    # model_tag = \"size:\" + str(100.0*float(param[\"sf\"]))+\"_aug:\"+ str(param['augment'])\n",
    "\n",
    "    model = load_model(model_file, delimiter=\"-\")\n",
    "    print(\"model loaded\")\n",
    "    metrics = evaluate_model(model, x_t, y_t)\n",
    "    print(\"metric computed\")\n",
    "    models_metrics[model_tag] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== aug 1 ===========\n",
      "\t1k\t10k\t50k\t100k\t\n",
      "mse\t0.014203454044915383\t0.002480648421910659\t0.0023629698830414827\t0.0023428905369572905\t\n",
      "mae\t0.07590188144170358\t0.023246562985204867\t0.022455827967532375\t0.022737672659383878\t\n",
      "dtw\t5.352905033503025\t2.508415924637376\t2.463039142605034\t2.469922672372537\t\n",
      "dfd\t0.3573711908340173\t0.1159301913118411\t0.11344960920497135\t0.11383532855574399\t\n",
      "========= aug 0 ==========\n",
      "mse\t0.026081953403375113\t0.0024354813607664204\t0.002241024889381613\t0.002290856997408402\t\n",
      "mae\t0.11063311492115666\t0.02347701585593545\t0.021051997831146235\t0.022014576386467935\t\n",
      "dtw\t8.207000090290196\t2.470160787666334\t2.3282010349645272\t2.3930190131558344\t\n",
      "dfd\t0.46488338701590753\t0.11683638486097525\t0.10978990013544507\t0.11175631639873772\t\n"
     ]
    }
   ],
   "source": [
    "print(\"========== aug 1 ===========\\n\\t\",end=\"\")\n",
    "for m in [\"1k\",\"10k\",\"50k\",\"100k\"]:\n",
    "    print(m, end=\"\\t\")\n",
    "print()\n",
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "    for n,m in models_metrics.items():\n",
    "        if \"aug:1\" in n:\n",
    "            print(m[k], end=\"\\t\")\n",
    "    print()\n",
    "print(\"========= aug 0 ==========\")\n",
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "    for n,m in models_metrics.items():\n",
    "        if \"aug:0\" in n:\n",
    "            print(m[k], end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num of encoders, decoder and model depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================\n",
      "num Encoders:  1 \tnum Decoders:  3 \tDepth:  256\n",
      "{'num_layers_enc': 1, 'num_layers_dec': 3, 'd_model': 256, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:1-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 9s 12ms/step - loss: 1.4763e-04\n",
      "MSE:  0.0001494946627644822\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [04:16<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.002596976841045097\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.12208694415308749\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.73886212261568\n",
      "mae:\t 0.02640541335892475\n",
      "mse:\t 0.0025969768410450877\n",
      "======================================================================================================\n",
      "num Encoders:  1 \tnum Decoders:  3 \tDepth:  400\n",
      "{'num_layers_enc': 1, 'num_layers_dec': 3, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:1-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 10s 15ms/step - loss: 1.2021e-04\n",
      "MSE:  0.00012197512842249125\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [04:57<00:00,  7.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0027276886623445857\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.12122696939703666\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.702918660119998\n",
      "mae:\t 0.02529149711046403\n",
      "mse:\t 0.002727688662344588\n",
      "======================================================================================================\n",
      "num Encoders:  1 \tnum Decoders:  5 \tDepth:  256\n",
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 256, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:1-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 13s 18ms/step - loss: 1.5300e-04\n",
      "MSE:  0.00015453933156095445\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [06:12<00:00,  9.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.002726531303711237\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.12806367503062693\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.913398924493158\n",
      "mae:\t 0.02838061532357939\n",
      "mse:\t 0.0027265313037112566\n",
      "======================================================================================================\n",
      "num Encoders:  1 \tnum Decoders:  5 \tDepth:  400\n",
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 15s 22ms/step - loss: 9.7113e-05\n",
      "MSE:  9.826025780057535e-05\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [07:14<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.002451821950968988\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.11872049748753903\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.602604161995969\n",
      "mae:\t 0.024537796733149944\n",
      "mse:\t 0.0024518219509689866\n",
      "======================================================================================================\n",
      "num Encoders:  2 \tnum Decoders:  3 \tDepth:  256\n",
      "{'num_layers_enc': 2, 'num_layers_dec': 3, 'd_model': 256, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:2-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 10s 14ms/step - loss: 1.6059e-04\n",
      "MSE:  0.00016342209710273892\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [05:01<00:00,  7.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0028254162351774624\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.13036128186189053\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.981241223477885\n",
      "mae:\t 0.030203137270130145\n",
      "mse:\t 0.0028254162351774585\n",
      "======================================================================================================\n",
      "num Encoders:  2 \tnum Decoders:  3 \tDepth:  400\n",
      "{'num_layers_enc': 2, 'num_layers_dec': 3, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:2-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 12s 17ms/step - loss: 1.2579e-04\n",
      "MSE:  0.00012719212099909782\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [05:50<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.002568349239366221\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.12146609359756542\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.7679229428692076\n",
      "mae:\t 0.027026672595851592\n",
      "mse:\t 0.0025683492393662063\n",
      "======================================================================================================\n",
      "num Encoders:  2 \tnum Decoders:  5 \tDepth:  256\n",
      "{'num_layers_enc': 2, 'num_layers_dec': 5, 'd_model': 256, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:2-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 14s 20ms/step - loss: 1.6047e-04\n",
      "MSE:  0.00016268411127384752\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [06:49<00:00, 10.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.00282736868302177\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.13193735104718837\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 3.0200743898124935\n",
      "mae:\t 0.03050950657869114\n",
      "mse:\t 0.002827368683021769\n",
      "======================================================================================================\n",
      "num Encoders:  2 \tnum Decoders:  5 \tDepth:  400\n",
      "{'num_layers_enc': 2, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 1.0, 'augment': 1}\n",
      "loading weights:  /home/azureuser/data/models/ICRA_TF4D_enc_dec_depth/TF-num_layers_enc:2-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "605/605 [==============================] - 17s 24ms/step - loss: 9.9642e-05\n",
      "MSE:  0.00010097025369759649\n",
      "---------------------------------------------------\n",
      "with autoregressive generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [08:02<00:00, 12.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0026205959150691696\n",
      "Trajectory metrics:\n",
      "pcm:\t 0.0\n",
      "dfd:\t 0.1242314966561472\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.7840958375592493\n",
      "mae:\t 0.02682884721046181\n",
      "mse:\t 0.0026205959150691523\n"
     ]
    }
   ],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_enc_dec_depth/\"\n",
    "\n",
    "model_names =  [\n",
    "\"TF-num_layers_enc:1-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "]\n",
    "models_metrics_enc_dec = {}\n",
    "for model_name in model_names:\n",
    "\n",
    "    model_file = model_path+model_name\n",
    "    param = file_name2dict(model_file,delimiter=\"-\",show=False)\n",
    "    print(\"======================================================================================================\")\n",
    "    print(\"num Encoders: \",param[\"num_layers_enc\"],\"\\tnum Decoders: \",param[\"num_layers_dec\"],\"\\tDepth: \",param['d_model'])\n",
    "    model_tag = \"enc:\"+str(param[\"num_layers_enc\"])+\"-dec:\"+str(param[\"num_layers_dec\"])+\"-d\"+str(param['d_model'])\n",
    "\n",
    "    model = load_model(model_file, delimiter=\"-\")\n",
    "    metrics = evaluate_model(model, x_t, y_t)\n",
    "    models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\tmae\tdtw\tdfd\t\n",
      "enc:1-dec:3-d256\t0.0025969768410450877\t0.02640541335892475\t2.73886212261568\t0.12208694415308749\t\n",
      "enc:1-dec:3-d400\t0.002727688662344588\t0.02529149711046403\t2.702918660119998\t0.12122696939703666\t\n",
      "enc:1-dec:5-d256\t0.0027265313037112566\t0.02838061532357939\t2.913398924493158\t0.12806367503062693\t\n",
      "enc:1-dec:5-d400\t0.0024518219509689866\t0.024537796733149944\t2.602604161995969\t0.11872049748753903\t\n",
      "enc:2-dec:3-d256\t0.0028254162351774585\t0.030203137270130145\t2.981241223477885\t0.13036128186189053\t\n",
      "enc:2-dec:3-d400\t0.0025683492393662063\t0.027026672595851592\t2.7679229428692076\t0.12146609359756542\t\n",
      "enc:2-dec:5-d256\t0.002827368683021769\t0.03050950657869114\t3.0200743898124935\t0.13193735104718837\t\n",
      "enc:2-dec:5-d400\t0.0026205959150691523\t0.02682884721046181\t2.7840958375592493\t0.1242314966561472\t\n"
     ]
    }
   ],
   "source": [
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "print()\n",
    "for n,m in models_metrics.items():\n",
    "    if \"dec\" in n:\n",
    "        print(n, end=\"\\t\")\n",
    "        for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "            print(m[k], end=\"\\t\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive predictor\n",
    "Coping initial trajectory (no modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "loading dataset:  latte_100k_lf ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (96718, 953) \tY: (96718, 160)\n",
      "Train X: (67702, 953) \tY: (67702, 160)\n",
      "Test  X: (19344, 953) \tY: (19344, 160)\n",
      "Val   X: (9672, 953) \tY: (9672, 160)\n"
     ]
    }
   ],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19344it [08:34, 37.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcm:\t 0.0\n",
      "dfd:\t 0.13866536138405858\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 3.5680424145512766\n",
      "mae:\t 0.027094356069811483\n",
      "mse:\t 0.004370182933815549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_tag=\"naive_predictor\"\n",
    "traj_in = x_t[0][:,MAX_NUM_OBJS+1:,:].numpy()\n",
    "traj_out = y_t.numpy()\n",
    "metrics, metrics_h = compute_metrics(traj_out,traj_in)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "clip only\n",
      "loading dataset:  latte_100k_lf_cliponly ...done\n",
      "raw X: (100000, 697) \tY: (100000, 160)\n",
      "filtered X: (96718, 697) \tY: (96718, 160)\n",
      "Train X: (67702, 697) \tY: (67702, 160)\n",
      "Test  X: (19344, 697) \tY: (19344, 160)\n",
      "Val   X: (9672, 697) \tY: (9672, 160)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=True)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_cliponly\", filter_data = True, base_path=data_folder)\n",
    "X[:,feature_indices] = 0\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_no_language/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"no_language\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [07:18<00:00, 11.25s/it]\n"
     ]
    }
   ],
   "source": [
    "pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"test_no_language_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "loading dataset:  latte_100k_lf ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (96718, 953) \tY: (96718, 160)\n",
      "Train X: (67702, 953) \tY: (67702, 160)\n",
      "Test  X: (19344, 953) \tY: (19344, 160)\n",
      "Val   X: (9672, 953) \tY: (9672, 160)\n"
     ]
    }
   ],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 16, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': True, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse'}\n",
      "loading weights:  /home/azureuser/data/models/FINAL_decoder_only/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\n",
      "\n",
      "with next waypoint prediction\n",
      "---------------------------------------------------\n",
      "Trajectory metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19344it [09:06, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcm:\t 0.0\n",
      "dfd:\t 0.12620982754684149\n",
      "area:\t 0.0\n",
      "cl:\t 0.0\n",
      "dtw:\t 2.290391868346655\n",
      "mae:\t 0.02421539465031163\n",
      "mse:\t 0.0016075996776673247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1903644/3192676914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_dec_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_t_deconly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodels_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_tag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'models_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "from src.TF4D_decoder_only import *\n",
    "model_path = models_folder+\"FINAL_decoder_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\"\n",
    "\n",
    "def evaluate_model_dec_only(model, x_t, y_t):\n",
    "\n",
    "    print(\"\\nwith next waypoint prediction\")\n",
    "    # result_eval = model.evaluate(x_t,y_t)\n",
    "    # print(\"MSE: \",result_eval)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    pred = model.predict(x_t)\n",
    "    print(\"Trajectory metrics:\")\n",
    "    metrics, metrics_h = compute_metrics(y_t.numpy()[:,:,:],pred[:,:,:])\n",
    "    return metrics\n",
    "\n",
    "x_t_deconly = (x_t[0][:,MAX_NUM_OBJS:-1,:],x_t[1],x_t[2])\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model_dec_only(model,x_t_deconly, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_t_deconly)\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"test_decoder_only_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "clip only\n",
      "loading dataset:  latte_100k_lf_cliponly ...done\n",
      "raw X: (100000, 697) \tY: (100000, 160)\n",
      "filtered X: (96718, 697) \tY: (96718, 160)\n",
      "Train X: (67702, 697) \tY: (67702, 160)\n",
      "Test  X: (19344, 697) \tY: (19344, 160)\n",
      "Val   X: (9672, 697) \tY: (9672, 160)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=True)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_cliponly\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_clip_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "loading dataset:  latte_100k_lf_textonly ...done\n",
      "raw X: (100000, 953) \tY: (100000, 160)\n",
      "filtered X: (96718, 953) \tY: (96718, 160)\n",
      "Train X: (67702, 953) \tY: (67702, 160)\n",
      "Test  X: (19344, 953) \tY: (19344, 160)\n",
      "Val   X: (9672, 953) \tY: (9672, 160)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_textonly\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_text_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BERT model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "loading CLIP model... done\n",
      "DEVICE:  cuda\n",
      "loading dataset:  forces_only_f ...done\n",
      "raw X: (10000, 953) \tY: (10000, 160)\n",
      "filtered X: (10000, 953) \tY: (10000, 160)\n",
      "Train X: (7000, 953) \tY: (7000, 160)\n",
      "Test  X: (2000, 953) \tY: (2000, 160)\n",
      "Val   X: (1000, 953) \tY: (1000, 160)\n",
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 16, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': True, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse'}\n",
      "loading weights:  /home/azureuser/data/models/forces_onl/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"forces_only_f\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "\n",
    "def prepare_x(x):\n",
    "    objs = pad_array(list_to_wp_seq(x[:,obj_poses_indices],d=3),4,axis=-1) # no speed\n",
    "    trajs = list_to_wp_seq(x[:,traj_indices],d=4)\n",
    "    #   return np.concatenate([objs,trajs],axis = 1)\n",
    "    return trajs[:,:-1,:]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 16, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': True, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse'}\n",
      "loading weights:  /home/azureuser/data/models/forces_onl/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\n"
     ]
    }
   ],
   "source": [
    "from src.TF4D_decoder_only import *\n",
    "\n",
    "model_path = models_folder+\"forces_onl/\"\n",
    "model_name = \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\"\n",
    "\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "\n",
    "# metrics = evaluate_model(model, x_t, y_t)\n",
    "# models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_t)\n",
    "pred_d = np.array(data)[indices_test]\n",
    "for i,d in enumerate(pred_d):\n",
    "    d[\"forces\"] = pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "indices = np.random.choice(range(len(indices_test)), 3)\n",
    "\n",
    "plt.close('all')\n",
    "# pred_t = np.transpose(pred[:,:,:2],[0,2,1])\n",
    "data_array = np.array(data)[indices_test[indices]]\n",
    "show_data4D(data_array,plot_output=False)\n",
    "# data_array = pred_d[indices]\n",
    "# show_data4D(data_array,plot_output=False)\n",
    "\n",
    "# show_data4D(data_array,plot_output=False,image_loader=mr.image_loader, color_traj=False, change_img_base=[\"/home/mirmi/Arthur/dataset/\",\"/home/tum/data/image_dataset/\"])\n",
    "# show_data4D(data_sample,plot_output=False)#,image_loader=mr.image_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers_enc': 1, 'num_layers_dec': 5, 'd_model': 400, 'dff': 512, 'num_heads': 8, 'dropout_rate': 0.1, 'wp_d': 4, 'num_emb_vec': 4, 'bs': 16, 'dense_n': 512, 'num_dense': 3, 'concat_emb': False, 'features_n': 793, 'optimizer': 'adam', 'norm_layer': True, 'activation': 'tanh', 'loss': 'mse', 'sf': 0.5, 'augment': 1}\n",
      "loading weights:  /home/arthur/local_data/models/FINAL_dataset_size_aug_fixsteps/TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:1.h5\n"
     ]
    }
   ],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_dataset_size_aug_fixsteps/\"\n",
    "\n",
    "model_name = \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:1.h5\"\n",
    "model_file = model_path+model_name\n",
    "\n",
    "model = load_model(model_file, delimiter=\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"testpred_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.random.choice(range(len(indices_test)), 3)\n",
    "indices = np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = mr.load_data(\"testpred_100k_latte_f\", base_path=data_folder)\n",
    "pred = np.array([d[\"output_traj\"] for d in data_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.functions import *\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# plot best results\n",
    "th = 0.0005 # maximum mse\n",
    "good_indices = {\"dist\":[], \"cartesian\":[], \"speed\":[]}\n",
    "for i,d in enumerate(np.array(data)[indices_test]):\n",
    "    mse = np.mean((pred[i]-np.array(d[\"output_traj\"]))**2)\n",
    "    if mse < th and len(d[\"obj_names\"])<4:\n",
    "        good_indices[d[\"change_type\"]] = good_indices[d[\"change_type\"]] + [i]\n",
    "indices = [np.random.choice(good_indices[i]) for i in good_indices.keys()]\n",
    "indices[0] =6276#11699#,11664\n",
    "# indices[1] =3190\n",
    "# indices[2] = 14168#11053, 16036, 14168\n",
    "indices = indices\n",
    "print(indices)\n",
    "\n",
    "data_array = np.array(data)[indices_test[indices]]\n",
    "\n",
    "show_data4D(data_array, pred=pred[indices,:,:],color_traj=False,labels=[\"Ground Truth\",\"Predicted\"],image_loader=mr.image_loader, change_img_base=[\"/mnt/tumdata/image_dataset/\", image_dataset_folder])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_cu11_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de91200c9f9e1f8a0c28ceba668014be0fd55838e84400e0a7ad1d269192773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
