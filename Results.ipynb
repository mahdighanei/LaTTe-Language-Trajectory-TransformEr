{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mghanei3/miniconda3/envs/llm/lib/python3.8/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-04-25 13:54:24.683010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import os # I think for cpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "def evaluate_model(model, x_t, y_t):\n",
    "\n",
    "    print(\"\\nwith next waypoint prediction\")\n",
    "    result_eval = model.evaluate(x_t,y_t)\n",
    "    print(\"MSE: \",result_eval)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"with autoregressive generation:\")\n",
    "\n",
    "    pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "    result_gen = np.average((y_t - pred[:,1:,:])**2)\n",
    "    print(\"MSE: \",result_gen)\n",
    "    print(\"Trajectory metrics:\")\n",
    "    metrics, metrics_h = compute_metrics(y_t.numpy()[:,:,:],pred[:,1:,:])\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset size experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[0])\n",
    "# print(X_test.shape)\n",
    "import pandas as pd\n",
    "foo = pd.read_json(\"data/data.json\")\n",
    "print(foo)\n",
    "print(foo.iloc[0]['obj_poses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "loading weights:  models/1st_attempt/TF&num_layers_enc:1&num_layers_dec:6&d_model:256&dff:512&num_heads:8&dropout_rate:0.1&wp_d:2&bs:64&dense_n:512&num_dense:3&concat_emb:True&features_n:793&optimizer:adam&norm_layer:True&activation:tanh.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer #0 (named \"encoder_2\" in the current model) was found to correspond to layer encoder in the save file. However the new layer encoder_2 expects 34 weights, but the saved weights have 18 elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m model_file \u001b[38;5;241m=\u001b[39m model_path\u001b[38;5;241m+\u001b[39mmodel_name\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# param = file_name2dict(model_file,delimiter=\"-\",show=False)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"======================================================================================================\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"Dataset size: \",str(100.0*float(param[\"sf\"])),\"%\\t\",\"augmentation: \",\"ON \"if param['augment']==1 else \"OFF\", \"\\n\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# model_tag = \"size:\" + str(100.0*float(param[\"sf\"]))+\"_aug:\"+ str(param['augment'])\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, x_t, y_t)\n",
      "File \u001b[0;32m~/Mahdi/research/code/llmclass/LaTTe-Language-Trajectory-TransformEr/src/simple_TF_continuos.py:494\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_file, model_path, delimiter)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# model.load_weights(os.path.join(model_path,f))\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading weights: \u001b[39m\u001b[38;5;124m\"\u001b[39m,model_file)\n\u001b[0;32m--> 494\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2234\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2231\u001b[0m   hdf5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group_by_name(\n\u001b[1;32m   2232\u001b[0m       f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch)\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2234\u001b[0m   \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py:702\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    699\u001b[0m   weight_values \u001b[38;5;241m=\u001b[39m preprocess_weights_for_loading(\n\u001b[1;32m    700\u001b[0m       layer, weight_values, original_keras_version, original_backend)\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(symbolic_weights):\n\u001b[0;32m--> 702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer #\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    703\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the current model) was found to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    704\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrespond to layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in the save file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    705\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHowever the new layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    706\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(symbolic_weights)) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    707\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m weights, but the saved weights have \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    708\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weight_values)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m elements.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    709\u001b[0m   weight_value_tuples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values)\n\u001b[1;32m    710\u001b[0m K\u001b[38;5;241m.\u001b[39mbatch_set_value(weight_value_tuples)\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #0 (named \"encoder_2\" in the current model) was found to correspond to layer encoder in the save file. However the new layer encoder_2 expects 34 weights, but the saved weights have 18 elements."
     ]
    }
   ],
   "source": [
    "# from src.TF4D_mult_features import * # Original - commented out\n",
    "from src.simple_TF_continuos import *\n",
    "# model_path = models_folder+\"FINAL_dataset_size_aug_fixsteps/\"\n",
    "models_folder = \"models/\"\n",
    "model_path = models_folder\n",
    "\n",
    "# model_names =  [\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.01-augment:0.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.01-augment:1.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.1-augment:0.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.1-augment:1.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:0.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:1.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:0.h5\",\n",
    "#                 \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"]\n",
    "\n",
    "model_names = [\"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh.h5\"]\n",
    "model_names = [\"1st_attempt/TF&num_layers_enc:1&num_layers_dec:6&d_model:256&dff:512&num_heads:8&dropout_rate:0.1&wp_d:2&bs:64&dense_n:512&num_dense:3&concat_emb:True&features_n:793&optimizer:adam&norm_layer:True&activation:tanh.h5\"]\n",
    "\n",
    "models_metrics = {}\n",
    "for model_name in model_names:\n",
    "\n",
    "    model_file = model_path+model_name\n",
    "    # param = file_name2dict(model_file,delimiter=\"-\",show=False)\n",
    "    # print(\"======================================================================================================\")\n",
    "    # print(\"Dataset size: \",str(100.0*float(param[\"sf\"])),\"%\\t\",\"augmentation: \",\"ON \"if param['augment']==1 else \"OFF\", \"\\n\")\n",
    "    # model_tag = \"size:\" + str(100.0*float(param[\"sf\"]))+\"_aug:\"+ str(param['augment'])\n",
    "\n",
    "    model = load_model(model_file, delimiter=\"-\")\n",
    "    print(\"model loaded\")\n",
    "    metrics = evaluate_model(model, x_t, y_t)\n",
    "    print(\"metric computed\")\n",
    "    models_metrics[model_tag] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========== aug 1 ===========\\n\\t\",end=\"\")\n",
    "for m in [\"1k\",\"10k\",\"50k\",\"100k\"]:\n",
    "    print(m, end=\"\\t\")\n",
    "print()\n",
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "    for n,m in models_metrics.items():\n",
    "        if \"aug:1\" in n:\n",
    "            print(m[k], end=\"\\t\")\n",
    "    print()\n",
    "print(\"========= aug 0 ==========\")\n",
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "    for n,m in models_metrics.items():\n",
    "        if \"aug:0\" in n:\n",
    "            print(m[k], end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num of encoders, decoder and model depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_enc_dec_depth/\"\n",
    "\n",
    "model_names =  [\n",
    "\"TF-num_layers_enc:1-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:3-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:3-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:5-d_model:256-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\",\n",
    "\"TF-num_layers_enc:2-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "]\n",
    "models_metrics_enc_dec = {}\n",
    "for model_name in model_names:\n",
    "\n",
    "    model_file = model_path+model_name\n",
    "    param = file_name2dict(model_file,delimiter=\"-\",show=False)\n",
    "    print(\"======================================================================================================\")\n",
    "    print(\"num Encoders: \",param[\"num_layers_enc\"],\"\\tnum Decoders: \",param[\"num_layers_dec\"],\"\\tDepth: \",param['d_model'])\n",
    "    model_tag = \"enc:\"+str(param[\"num_layers_enc\"])+\"-dec:\"+str(param[\"num_layers_dec\"])+\"-d\"+str(param['d_model'])\n",
    "\n",
    "    model = load_model(model_file, delimiter=\"-\")\n",
    "    metrics = evaluate_model(model, x_t, y_t)\n",
    "    models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "    print(k, end=\"\\t\")\n",
    "print()\n",
    "for n,m in models_metrics.items():\n",
    "    if \"dec\" in n:\n",
    "        print(n, end=\"\\t\")\n",
    "        for k in [\"mse\",\"mae\",\"dtw\",\"dfd\"]:\n",
    "            print(m[k], end=\"\\t\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive predictor\n",
    "Coping initial trajectory (no modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag=\"naive_predictor\"\n",
    "traj_in = x_t[0][:,MAX_NUM_OBJS+1:,:].numpy()\n",
    "traj_out = y_t.numpy()\n",
    "metrics, metrics_h = compute_metrics(traj_out,traj_in)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=True)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_cliponly\", filter_data = True, base_path=data_folder)\n",
    "X[:,feature_indices] = 0\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_no_language/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"no_language\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"test_no_language_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_decoder_only import *\n",
    "model_path = models_folder+\"FINAL_decoder_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\"\n",
    "\n",
    "def evaluate_model_dec_only(model, x_t, y_t):\n",
    "\n",
    "    print(\"\\nwith next waypoint prediction\")\n",
    "    # result_eval = model.evaluate(x_t,y_t)\n",
    "    # print(\"MSE: \",result_eval)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    pred = model.predict(x_t)\n",
    "    print(\"Trajectory metrics:\")\n",
    "    metrics, metrics_h = compute_metrics(y_t.numpy()[:,:,:],pred[:,:,:])\n",
    "    return metrics\n",
    "\n",
    "x_t_deconly = (x_t[0][:,MAX_NUM_OBJS:-1,:],x_t[1],x_t[2])\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model_dec_only(model,x_t_deconly, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_t_deconly)\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"test_decoder_only_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clip only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=True)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_cliponly\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_clip_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"latte_100k_lf_textonly\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((mr.prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_text_only/\"\n",
    "\n",
    "model_name=  \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:537-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:1.0-augment:1.h5\"\n",
    "model_tag=\"decoder_only\"\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "metrics = evaluate_model(model, x_t, y_t)\n",
    "models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from src.motion_refiner_4D import Motion_refiner, MAX_NUM_OBJS\n",
    "from src.config import *\n",
    "from src.functions import *\n",
    "\n",
    "mr = Motion_refiner(load_models=True ,traj_n = traj_n, locality_factor=True, clip_only=False)\n",
    "feature_indices, obj_sim_indices, obj_poses_indices, traj_indices = mr.get_indices()\n",
    "embedding_indices = mr.embedding_indices\n",
    "\n",
    "#============================== load dataset ==========================================\n",
    "X,Y, data = mr.load_dataset(\"forces_only_f\", filter_data = True, base_path=data_folder)\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, indices_train, indices_test, indices_val = mr.split_dataset(X, Y, test_size=0.2, val_size=0.1)\n",
    "\n",
    "\n",
    "def prepare_x(x):\n",
    "    objs = pad_array(list_to_wp_seq(x[:,obj_poses_indices],d=3),4,axis=-1) # no speed\n",
    "    trajs = list_to_wp_seq(x[:,traj_indices],d=4)\n",
    "    #   return np.concatenate([objs,trajs],axis = 1)\n",
    "    return trajs[:,:-1,:]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((prepare_x(X_test),\n",
    "                                                    list_to_wp_seq(y_test,d=4),\n",
    "                                                    X_test[:,embedding_indices])).batch(X_test.shape[0])\n",
    "\n",
    "g = generator(test_dataset,stop=True,augment=False)\n",
    "x_t, y_t = next(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_decoder_only import *\n",
    "\n",
    "model_path = models_folder+\"forces_onl/\"\n",
    "model_name = \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:16-bs:16-dense_n:512-num_dense:3-concat_emb:True-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse.h5\"\n",
    "\n",
    "model_file = model_path+model_name\n",
    "model = load_model(model_file, delimiter=\"-\")\n",
    "\n",
    "# metrics = evaluate_model(model, x_t, y_t)\n",
    "# models_metrics[model_tag] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_t)\n",
    "pred_d = np.array(data)[indices_test]\n",
    "for i,d in enumerate(pred_d):\n",
    "    d[\"forces\"] = pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "indices = np.random.choice(range(len(indices_test)), 3)\n",
    "\n",
    "plt.close('all')\n",
    "# pred_t = np.transpose(pred[:,:,:2],[0,2,1])\n",
    "data_array = np.array(data)[indices_test[indices]]\n",
    "show_data4D(data_array,plot_output=False)\n",
    "# data_array = pred_d[indices]\n",
    "# show_data4D(data_array,plot_output=False)\n",
    "\n",
    "# show_data4D(data_array,plot_output=False,image_loader=mr.image_loader, color_traj=False, change_img_base=[\"/home/mirmi/Arthur/dataset/\",\"/home/tum/data/image_dataset/\"])\n",
    "# show_data4D(data_sample,plot_output=False)#,image_loader=mr.image_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TF4D_mult_features import *\n",
    "model_path = models_folder+\"FINAL_dataset_size_aug_fixsteps/\"\n",
    "\n",
    "model_name = \"TF-num_layers_enc:1-num_layers_dec:5-d_model:400-dff:512-num_heads:8-dropout_rate:0.1-wp_d:4-num_emb_vec:4-bs:16-dense_n:512-num_dense:3-concat_emb:False-features_n:793-optimizer:adam-norm_layer:True-activation:tanh-loss:mse-sf:0.5-augment:1.h5\"\n",
    "model_file = model_path+model_name\n",
    "\n",
    "model = load_model(model_file, delimiter=\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate(model ,x_t, traj_n=traj_n).numpy()\n",
    "data_pred = np.array(data)[indices_test].copy()\n",
    "for i,d in enumerate(data_pred):\n",
    "    d[\"output_traj\"] = pred[i]\n",
    "\n",
    "mr.save_data(data_pred,data_name=\"testpred_100k_latte_f\", base_path=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.random.choice(range(len(indices_test)), 3)\n",
    "indices = np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = mr.load_data(\"testpred_100k_latte_f\", base_path=data_folder)\n",
    "pred = np.array([d[\"output_traj\"] for d in data_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.functions import *\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# plot best results\n",
    "th = 0.0005 # maximum mse\n",
    "good_indices = {\"dist\":[], \"cartesian\":[], \"speed\":[]}\n",
    "for i,d in enumerate(np.array(data)[indices_test]):\n",
    "    mse = np.mean((pred[i]-np.array(d[\"output_traj\"]))**2)\n",
    "    if mse < th and len(d[\"obj_names\"])<4:\n",
    "        good_indices[d[\"change_type\"]] = good_indices[d[\"change_type\"]] + [i]\n",
    "indices = [np.random.choice(good_indices[i]) for i in good_indices.keys()]\n",
    "indices[0] =6276#11699#,11664\n",
    "# indices[1] =3190\n",
    "# indices[2] = 14168#11053, 16036, 14168\n",
    "indices = indices\n",
    "print(indices)\n",
    "\n",
    "data_array = np.array(data)[indices_test[indices]]\n",
    "\n",
    "show_data4D(data_array, pred=pred[indices,:,:],color_traj=False,labels=[\"Ground Truth\",\"Predicted\"],image_loader=mr.image_loader, change_img_base=[\"/mnt/tumdata/image_dataset/\", image_dataset_folder])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_cu11_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de91200c9f9e1f8a0c28ceba668014be0fd55838e84400e0a7ad1d269192773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
